# üß∞ Installation Guide ‚Äì Setting up Kafnus

This document provides step-by-step instructions to prepare and launch the Kafnus system. It covers the required plugins, custom builds, Python environment, and Docker setup.

---


## 1. üì¶ Kafka Connect Plugins Setup

Kafka Connect now uses a custom-built Docker image with all required plugins included. You'll need to:

1. Build the required JARs (next section)
2. Place them in under `kafka-connect-custom/plugins/` in the same directory structure shown below.
3. Build the custom Kafka Connect image


### ‚úÖ Required JARs

```plaintext
kafka-connect-custom/plugins/
‚îú‚îÄ‚îÄ header-router/
‚îÇ   ‚îî‚îÄ‚îÄ header-router-1.0.0.jar
‚îú‚îÄ‚îÄ kafka-connect-jdbc/
‚îÇ   ‚îú‚îÄ‚îÄ kafka-connect-jdbc-10.7.0.jar
‚îÇ   ‚îî‚îÄ‚îÄ postgresql-42.7.1.jar
‚îú‚îÄ‚îÄ mongodb/
‚îÇ   ‚îú‚îÄ‚îÄ mongo-kafka-connect-1.10.0-confluent.jar
‚îÇ   ‚îú‚îÄ‚îÄ mongodb-driver-core-4.9.1.jar
‚îÇ   ‚îú‚îÄ‚îÄ mongodb-driver-sync-4.9.1.jar
‚îÇ   ‚îî‚îÄ‚îÄ bson-4.9.1.jar
‚îî‚îÄ‚îÄ mqtt-kafka-connect/
    ‚îî‚îÄ‚îÄ mqtt-kafka-connect-1.0-jar-with-dependencies.jar
```

Next section explains how to build all them. At the end of executing the procedure described in that section, your
`kafka-connect-custom/plugins/` directory should look like shown above.

**Setup Notes**:  
- Custom Kafka Connect and Faust images build automatically when running `docker-up.sh`  
- Plugin preparation (steps 1-4) is only required for:  
  - Initial system setup    
  - Adding new plugins
  - Modify existing plugins  

---

## 2. üî® Building Required JARs

> ‚úÖ Requires **Java 17+**, **Maven 3.6+**

You can check your version with:

```bash
mvn --version
java --version
```

---

### 2.1. HeaderRouter (Custom SMT)

From the root of the project:

```bash
cd kafka-connect-custom/src/header-router/
mvn clean package
mkdir -p ../../plugins
mkdir -p ../../plugins/header-router
cp target/header-router-1.0.0-jar-with-dependencies.jar ../../plugins/header-router/header-router-1.0.0.jar
```

---

### 2.2. MQTT Source Connector

Modified version with dependencies included:

```bash
cd ../mqtt-kafka-connect/
mvn clean package
mkdir -p ../../plugins/mqtt-kafka-connect/
cp target/mqtt-kafka-connect-1.0-jar-with-dependencies.jar ../../plugins/mqtt-kafka-connect/
```

---

### 2.3. Custom JDBC Connector with PostGIS Support

Do this inside `own-jdbc-connector/`, where the patch file is located. Based on [PR #1048](https://github.com/confluentinc/kafka-connect-jdbc/pull/1048):

```bash
cd ../own-jdbc-connector/
git clone https://github.com/confluentinc/kafka-connect-jdbc.git
cd kafka-connect-jdbc
git checkout v10.7.0
git apply ../postgis-support.patch
mvn clean package -DskipTests -Dcheckstyle.skip=true
mkdir -p ../../../plugins/kafka-connect-jdbc
cp target/kafka-connect-jdbc-10.7.0.jar ../../../plugins/kafka-connect-jdbc/
```

Download PostgreSQL driver (required)

```bash
wget https://repo1.maven.org/maven2/org/postgresql/postgresql/42.7.1/postgresql-42.7.1.jar -P ../../../plugins/kafka-connect-jdbc/
```

---

### üîó Download MongoDB Kafka Sink Connector and Dependencies

To use MongoDB with Kafka Connect, you need the MongoDB connector and its dependencies.

You can obtain the **MongoDB Kafka Connector** (`mongo-kafka-connect-1.10.0-confluent.jar`) from:

üëâ **[Confluent Hub ‚Äì MongoDB Kafka Connector](https://www.confluent.io/hub/mongodb/kafka-connect-mongodb)**

> üìå This project uses **version `1.10.0`**, but newer version (e.g., `1.16.0`) has been tested and `1.10+` versions are expected to work without issues.  
> ‚úÖ Alternatively, you can install it using the Confluent Hub CLI:

After downloading the `.zip` manually, extract it and copy the `.jar` from the `lib/` directory. You can use this commands from `kafka-connect-custom` directory, if `.zip` is present:

```bash
cd ../../..
unzip mongodb-kafka-connect-mongodb-1.10.0.zip
mkdir -p plugins/mongodb/
cp mongodb-kafka-connect-mongodb-1.10.0/lib/mongo-kafka-connect-1.10.0-confluent.jar plugins/mongodb/
```

Then download the required MongoDB driver JARs from Maven Central:

- [`mongodb-driver-core-4.9.1.jar`](https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-core/4.9.1/mongodb-driver-core-4.9.1.jar)
- [`mongodb-driver-sync-4.9.1.jar`](https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-sync/4.9.1/mongodb-driver-sync-4.9.1.jar)
- [`bson-4.9.1.jar`](https://repo1.maven.org/maven2/org/mongodb/bson/4.9.1/bson-4.9.1.jar)

```bash
cd plugins/mongodb/
wget https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-core/4.9.1/mongodb-driver-core-4.9.1.jar
wget https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-sync/4.9.1/mongodb-driver-sync-4.9.1.jar
wget https://repo1.maven.org/maven2/org/mongodb/bson/4.9.1/bson-4.9.1.jar
```

---

## 3. üì• Download External Tools (for Monitoring)

To use **Prometheus and Grafana** for monitoring Kafka Connect, you‚Äôll need to download the **JMX Prometheus Java Agent**.

Kafka Connect exposes JMX metrics, and this Java agent allows Prometheus to scrape them via HTTP.

Note: This plugin is required for the Dockerfile to build successfully. Even if monitoring is optional, the JMX Prometheus Java Agent must be downloaded and available to avoid build failures.

### üîß JMX Prometheus Agent

Download it to the `kafka-connect-custom/monitoring/` directory:

```bash
cd ../..
# Ensure no conflicting directory exists
rm -rf monitoring/jmx_prometheus_javaagent.jar

wget https://repo1.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/0.20.0/jmx_prometheus_javaagent-0.20.0.jar -O monitoring/jmx_prometheus_javaagent.jar
```

This file will be automatically mounted into the Kafka Connect container if monitoring is enabled.

The default port is `9100`, and the configuration file used is `kafka-connect-custom/monitoring/kafka-connect.yml`.

---

## 4. üêç Python Environment Setup

> ‚úÖ Requires **Python 3.11**

Recommended: use a virtual environment and install dependencies (from root):

```bash
cd tests_end2end/functional/
python3 -m venv pytests-venv
source pytests-venv/bin/activate
pip install -r requirements.txt
```

> Before running the tests, copy the example environment file:

```bash
cp .env.example .env
```

> Then, customize any necessary environment variables (e.g. USE_EXTERNAL_POSTGIS, E2E_MANUAL_INSPECTION, etc.) in .env.

---

## 5. üê≥ Launch Docker Environment

### üåê Kafka Network: `kafka-postgis-net`

All containers are connected to a common **external Docker network** named:

```yaml
networks:
  kafka-postgis-net:
    external: true
```

This is declared in all `docker-compose` files.

---

#### ‚úÖ If you do NOT have this network yet:

Hint: you can check if using `docker network ls`.

You must create it manually before running `docker compose`:

```bash
docker network create kafka-postgis-net
```

---

### üîå Connecting to PostGIS

You have two options:

---

‚úÖ **1. If you already have an external PostGIS instance running**

You must connect your PostGIS container to the shared Docker network:

```bash
docker network connect kafka-postgis-net your-postgis-container-name
```

> Replace `your-postgis-container-name` with the actual container name.

You must also define the `DBPATH_POSTGIS` environment variable, pointing to the host directory where your external PostGIS instance stores data:

```bash
export DBPATH_POSTGIS=/data/postgis
```

> ‚ö†Ô∏è **IMPORTANT**: Ensure this directory exists and is owned by UID 999 and GID 999 (commonly used by PostGIS). Otherwise, the container may fail to start:

```bash
sudo chown -R 999:999 ${DBPATH_POSTGIS}
```


‚úÖ **2. If you want to use the internal PostGIS container**

Uncomment the relevant line in `docker-up.sh` to include the internal PostGIS container.

Also define the same environment variable:

```bash
export DBPATH_POSTGIS=/data/postgis
```

Ensure that the directory exists and is writable by the container (UID/GID 999):

```bash
sudo chown -R 999:999 ${DBPATH_POSTGIS}
```

---

### üèÅ Startup

Now launch the environment:

```bash
cd docker/
./docker-up.sh
```

Check containers are running:

```bash
docker ps
```

You should see at least:

```plaintext
faust-stream
orion
kafka-connect
kafka
iot-postgis
mongo
mosquitto
```


---

## 6. üîå Check Kafka Connect Plugins

```bash
curl -s http://localhost:8083/connector-plugins | jq
```

Look for:

- `com.telefonica.kafnus.mqtt.MqttSourceConnector`
- `io.confluent.connect.jdbc.JdbcSinkConnector`
- `com.mongodb.kafka.connect.MongoSinkConnector`

---

## 7. ‚öôÔ∏è Register Kafka Connectors

Hint: Before registering the connectors, make sure the tests database has been created. This is explained in the next section.

```bash
cd sinks/

curl -X POST -H "Content-Type: application/json" --data @pg-sink-historic.json http://localhost:8083/connectors
curl -X POST -H "Content-Type: application/json" --data @pg-sink-lastdata.json http://localhost:8083/connectors
curl -X POST -H "Content-Type: application/json" --data @pg-sink-mutable.json http://localhost:8083/connectors
curl -X POST -H "Content-Type: application/json" --data @pg-sink-errors.json   http://localhost:8083/connectors
curl -X POST -H "Content-Type: application/json" --data @mqtt-source.json     http://localhost:8083/connectors
```

Hint: you can check that connectors have been correctly added listing them with:

```bash
curl -H "Accept: application/json" http://localhost:8083/connectors
```

However, note that the registration is not kept if docker containers are stopped.

---

## 8. üß™ Quick Manual Test (MQTT ‚Üí PostGIS)

You must have:

- A database: `tests`
- A schema: `test`
- A table: `test.simple_sensor`

Hint: you can get a PG command interface in the container using `docker exec -it <container name> psql -U postgres` in the host

```sql
-- Assuming database 'tests' already exist, typically CREATE DATABASE tests;
\c tests
CREATE SCHEMA IF NOT EXISTS test;
CREATE TABLE test.simple_sensor (
  recvtime TIMESTAMPTZ,
  fiwareservicepath TEXT,
  entityid TEXT,
  entitytype TEXT,
  timeinstant TIMESTAMPTZ,
  temperature DOUBLE PRECISION
);
```

---

### 8.1. Create Orion Subscription

```bash
curl -X POST http://localhost:1026/v2/subscriptions \
  -H "Content-Type: application/json" \
  -H "fiware-service: test" \
  -H "fiware-servicepath: /simple" \
  -d '{
    "description": "Suscripci√≥n MQTT para datos de prueba",
    "subject": {
        "entities": [{ "idPattern": ".*", "type": "Sensor" }],
        "condition": { "attrs": [ "TimeInstant" ] }
    },
    "notification": {
        "mqttCustom": {
            "url": "mqtt://mosquitto:1883",
            "topic": "kafnus/test/simple/raw_historic"
        },
        "attrs": ["TimeInstant", "temperature"]
    }
  }'
```

Hint: you can check the subscription has been correctly created executing `curl -H 'fiware-service: test' -H 'fiware-servicepath: /simple' http://localhost:1026/v2/subscriptions`

---

### 8.2. Trigger a Notification

```bash
curl -X POST http://localhost:1026/v2/entities?options=upsert,forcedUpdate \
  -H "Content-Type: application/json" \
  -H "fiware-service: test" \
  -H "fiware-servicepath: /simple" \
  -d '{
    "id": "Sensor1",
    "type": "Sensor",
    "TimeInstant": {
      "type": "DateTime",
      "value": "2025-07-01T11:00:00Z"
    },
    "temperature": {"value": 26.0, "type": "Float"}
  }'
```

Hint: you can repeat the above command several times to send additional notifications.

You should now see data in the `test.simple_sensor` table in PostGIS, like this:

```
tests=# SELECT * FROM test.simple_sensor;

          recvtime          | fiwareservicepath | entityid | entitytype |      timeinstant       | temperature 
----------------------------+-------------------+----------+------------+------------------------+-------------
 2025-07-03 12:23:53.926+00 | simple            | Sensor1  | Sensor     | 2025-07-01 11:00:00+00 |          26
 2025-07-03 13:54:22.45+00  | simple            | Sensor1  | Sensor     | 2025-07-01 11:00:00+00 |          26
(2 rows)
```

> üí° For a full test, refer to the [08_testing.md](./08_testing.md) guide.

---

## 9. üßπ Shut Down

To stop all services:

```bash
cd docker/
./docker-down.sh
```

---

## ‚úÖ Summary

At this point, you should have:

- All required `.jar` plugins correctly built and placed
- Docker services up and running
- Kafka Connect plugins verified
- Manual end-to-end flow from Orion to PostGIS working
- Python tooling installed for further testing

---

## üß≠ Navigation

- [‚¨ÖÔ∏è Previous: Overview](./00_overview.md)
- [üè† Main index](../README.md#documentation)
- [‚û°Ô∏è Next: Architecture](./02_architecture.md)
