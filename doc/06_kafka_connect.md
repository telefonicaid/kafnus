# 🔄 Kafka Connect Configuration and Plugins

This document explains how Kafka Connect is configured to persist NGSI notifications processed by Faust to different data sinks (PostGIS and MongoDB).

---


## ⚙️ Environment Setup

This project uses Docker Compose to orchestrate the multi-service environment, including Kafka Connect and other components.

**Important:**  
We override the default `docker-compose` CLI used by some tools and libraries to instead use the latest Docker Compose V2 syntax (`docker compose`). This ensures compatibility with the newest Docker CLI features and avoids issues related to the deprecated `docker-compose` command.

The custom Python fixture leverages this by substituting commands so that all compose operations run through `docker compose` (V2), not `docker-compose` (V1).

> ⚠️ **Note:** Legacy `docker-compose` command could be used, you can simply remove or disable the custom overriding class (`DockerCompose`) in the test fixtures in [`common_test.py`](../tests_end2end/functional/common_test.py). Doing so will revert to the default behavior.

---

## 🧩 Kafka Connect Plugins

Located under the `kafka-connect-custom/plugins/` directory:

### 1. JDBC Plugin for PostGIS

Path: `kafka-connect-custom/plugins/kafka-connect-jdbc/`

Includes:

- `kafka-connect-jdbc-10.7.0.jar`
- `postgresql-42.7.1.jar`

Used in:
- `pg-sink-historic.json`
- `pg-sink-lastdata.json`
- `pg-sink-mutable.json`
- `pg-sink-errors.json`

### 2. MongoDB Sink Plugin

Path: `kafka-connect-custom/plugins/mongodb/`

Includes:

- `mongo-kafka-connect-1.10.0-confluent.jar`
- MongoDB drivers (`bson`, `driver-core`, `driver-sync`)

> 🚧 Mongo support is experimental and may change.

### 3. Custom SMT – HeaderRouter

Path: `kafka-connect-custom/plugins/header-router`

A Java-based Single Message Transform (SMT) implemented in `HeaderRouter.java`. It rewrites the topic name based on a Kafka record header (e.g. `target_table`) set by Faust.

#### SMT Configuration Example

```json
"transforms": "HeaderRouter",
"transforms.HeaderRouter.type": "com.example.HeaderRouter",
"transforms.HeaderRouter.header.key": "target_table"
```

### 4. MQTT Source Plugin

**Path:** `kafka-connect-custom/plugins/mqtt-kafka-connect/`

This connector enables receiving data from an MQTT broker (Mosquitto) and publishing it to Kafka. It has been added **temporarily** to serve as a bridge so that the **Context Broker (CB)** can continue sending notifications via MQTT until native Kafka support is implemented in the CB.

Once the CB is updated to support Kafka natively, this connector will be **removed** from the architecture.

- **Connector config file**: `mqtt-source.json`
- **Kafka target topic**: defined in the config file
- **MQTT broker**: the connector subscribes to the MQTT topics generated by the CB
- **Purpose**: provides temporary compatibility between the CB and the Kafka-based pipeline

---

## 🗂️ Sink Configurations

The sink connectors are defined under the `sinks/` directory and are responsible for persisting data processed by Kafka (and Faust) to destination databases.

### Configuration files:

- `pg-sink-historic.json`:  
  - **Mode**: Insert-only  
  - Stores immutable historical data  
  - No upserts – every record is stored

- `pg-sink-lastdata.json`:  
  - **Mode**: Upsert  
  - Stores only the latest observation (per entity and attribute)  
  - Based on timestamp or unique identifiers

- `pg-sink-mutable.json`:  
  - **Mode**: Mutable upsert  
  - Designed for data that may change (e.g., device status)

- `pg-sink-errors.json`:  
  - **Mode**: DLQ (Dead Letter Queue)  
  - Captures failed records during transformation or persistence  
  - Useful for debugging and monitoring errors

Each file could include:
- Kafka topic configuration (`topics`)
- Destination table mapping (PostGIS)
- JDBC dialect (PostgreSQL)
- SMT transformations (e.g., `HeaderRouter`)
- Error handling logic

> ✅ Historic, lastdata and mutable connectors use the JDBC plugin and the custom `HeaderRouter` SMT to dynamically route data to specific tables.

---

## ▶️ Registering Connectors

From the `sinks/` directory, you can register each connector using `curl`:

```bash
curl -X POST http://localhost:8083/connectors \
  -H "Content-Type: application/json" \
  --data @pg-sink-historic.json
```

Repeat the same process for the other configuration files:

```bash
curl -X POST http://localhost:8083/connectors \
  -H "Content-Type: application/json" \
  --data @pg-sink-lastdata.json

curl -X POST http://localhost:8083/connectors \
  -H "Content-Type: application/json" \
  --data @pg-sink-mutable.json

curl -X POST http://localhost:8083/connectors \
  -H "Content-Type: application/json" \
  --data @pg-sink-errors.json

curl -X POST http://localhost:8083/connectors \
  -H "Content-Type: application/json" \
  --data @mqtt-source.json
```

> To confirm that the connectors were successfully registered, run:

```bash
curl -H "Accept: application/json" http://localhost:8083/connectors
```

> To check connector status:

```bash
curl -s http://localhost:8083/connectors/your-connector/status | jq
```

**Expected output (example):**

```json
[
  "pg-sink-historic",
  "pg-sink-lastdata",
  "pg-sink-mutable",
  "pg-sink-errors",
  "mqtt-source"
]
```

> 🔍 If a connector is missing, check the Kafka Connect logs (`docker logs kafka-connect`) to identify possible errors in loading or configuration.

---

## 🧪 Testing Sinks

You can verify data arrival using:

```bash
docker exec -it kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic YOUR_TOPIC_NAME \
  --from-beginning --max-messages 10
```

To confirm persistence, check tables in PostGIS or MongoDB after running the corresponding test input.

## Navegación

- [⬅️ Previous: Faust](/doc/05_faust.md)
- [🏠 Main index](../README.md#documentation)
- [➡️ Next: Monitoring](/doc/07_monitoring.md)